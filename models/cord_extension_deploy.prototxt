# Pretrained via softmax classifier; now stepping forward to Euclidean
name: "Unary depth regressor, euclidean version, deploy"

# use this style for running mode
input: "data"
input_dim: 1
input_dim: 3
input_dim: 227
input_dim: 227

input: "xy"
input_dim: 1
input_dim: 1
input_dim: 6
input_dim: 6


### PRE-TRAINED LAYERS ------------------------------------------
layers {
  name: "conv1"
  type: CONVOLUTION
  bottom: "data"
  top: "conv1"
  #param {
  #  lr_mult: 1
  #  decay_mult: 1
  #}
  #param {
  #  lr_mult: 2
  #  decay_mult: 0
  #}
  blobs_lr: 0          # learning rate multiplier for the filters
  blobs_lr: 0          # learning rate multiplier for the biases
  weight_decay: 0      # weight decay multiplier for the filters
  weight_decay: 0      # weight decay multiplier for the biases
  convolution_param {
    num_output: 96     # learn 96 filters
    kernel_size: 11    # each filter is 11x11
    stride: 4          # step 4 pixels between each filter application
    weight_filler {
      type: "gaussian" # initialize the filters from a Gaussian
      std: 0.01        # distribution with stdev 0.01 (default mean: 0)
    }
    bias_filler {
      type: "constant" # initialize the biases to zero (0)
      value: 0
    }
  }
}

layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layers {
  name: "pool1"
  type: POOLING
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  name: "norm1"
  type: LRN
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}

layers {
  name: "conv2"
  type: CONVOLUTION
  bottom: "norm1"
  top: "conv2"
  #param {
  #  lr_mult: 1
  #  decay_mult: 1
  #}
  #param {
  #  lr_mult: 2
  #  decay_mult: 0
  #}
  blobs_lr: 0          # learning rate multiplier for the filters
  blobs_lr: 0          # learning rate multiplier for the biases
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian" # initialize the filters from a Gaussian
      std: 0.01        # distribution with stdev 0.01 (default mean: 0)
    }
    bias_filler {
      type: "constant"
      value: 1.0
    }
  }
}
layers {
  name: "relu2"
  type: RELU
  bottom: "conv2"
  top: "conv2"
}
layers {
  name: "pool2"
  type: POOLING
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  name: "norm2"
  type: LRN
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}

layers {
  name: "conv3"
  type: CONVOLUTION
  bottom: "norm2"
  top: "conv3"
  blobs_lr: 0          # learning rate multiplier for the filters
  blobs_lr: 0          # learning rate multiplier for the biases
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
  }
}
layers {
  name: "relu3"
  type: RELU
  bottom: "conv3"
  top: "conv3"
}
layers {
  name: "conv4"
  type: CONVOLUTION
  bottom: "conv3"
  top: "conv4"
  blobs_lr: 0          # learning rate multiplier for the filters
  blobs_lr: 0          # learning rate multiplier for the biases
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
  }
}
layers {
  name: "relu4"
  type: RELU
  bottom: "conv4"
  top: "conv4"
}

layers {
  name: "conv5"
  type: CONVOLUTION
  bottom: "conv4"
  top: "conv5"
  blobs_lr: 0          # learning rate multiplier for the filters
  blobs_lr: 0          # learning rate multiplier for the biases  
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
  }
}
layers {
  name: "relu5"
  type: RELU
  bottom: "conv5"
  top: "conv5"
}

layers {
  name: "pool5"
  type: POOLING
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}

layers {
  name: "concat"
  bottom: "pool5"
  bottom: "xy"
  top: "concat"
  type: CONCAT
  concat_param {
    axis: 1
  }
}



### FULLY-CONNECTED LAYERS ------------------------------------------
layers {
  name: "fc6n"
  type: INNER_PRODUCT
  bottom: "concat"
  top: "fc6n"
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  blobs_lr: 1
  blobs_lr: 1
}
layers {
  name: "relu6n"
  type: RELU
  bottom: "fc6n"
  top: "fc6n"
}
layers {
  name: "drop6n"
  type: DROPOUT
  bottom: "fc6n"
  top: "fc6n"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  name: "fc7n"
  type: INNER_PRODUCT
  bottom: "fc6n"
  top: "fc7n"
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }

  blobs_lr: 1
  blobs_lr: 1
}
layers {
  name: "relu7n"
  type: RELU
  bottom: "fc7n"
  top: "fc7n"
}
layers {
  name: "drop7n"
  type: DROPOUT
  bottom: "fc7n"
  top: "fc7n"
  dropout_param {
    dropout_ratio: 0.5
  }
}


layers {
  name: "fc_n1"
  type: INNER_PRODUCT
  bottom: "fc7n"
  top: "fc_n1"
  inner_product_param {
    num_output: 128
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  blobs_lr: 1
  blobs_lr: 1
}
layers {
  name: "relu_n1"
  type: RELU
  bottom: "fc_n1"
  top: "fc_n1"
}
layers {
  name: "drop_n1"
  type: DROPOUT
  bottom: "fc_n1"
  top: "fc_n1"
  dropout_param {
    dropout_ratio: 0.5
  }

}

layers {
  name: "fc_n2"
  type: INNER_PRODUCT
  bottom: "fc_n1"
  top: "fc_n2"
  inner_product_param {
    num_output: 16
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  blobs_lr: 1
  blobs_lr: 1
}


# EUCLIDEAN LOSS LAYER / FOR REGRESSION
layers {
  name: "prob"
  type: SOFTMAX
  bottom: "fc_n2"
  top: "fc_n3"
}


layers {
  name: "output_neuron"
  type: INNER_PRODUCT
  bottom: "fc_n3"
  top: "output_neuron"
  inner_product_param {
    num_output: 1
  }
}
#layers {
#  name: "loss"
#  type: EUCLIDEAN_LOSS
#  bottom: "output_neuron"
#  bottom: "label"
#}


